https://github.com/bigknife789/asdasd/blob/main/032002206
# 一.psp表格
| PSP2.1     |Personal Software Process Stages     | 预估耗时（分钟）     |  实际耗时（分钟）     |
| -------- | -------- | -------- | -------- |
| Planning | 计划| 30 | 50 |
| · Estimate  | · 估计这个任务需要多少时间 | 1500| 2550 | 
| Development |开发| 60 | 60 |
 |· Analysis  |· 需求分析 (包括学习新技术) | 500 | 1000 |
 | · Design Spec |·生成设计文档| 30 | 30 |
 | · Design Review| · 设计复审| 30 | 30 |
 | · Coding Standard  | · 代码规范(为目前的开发制定合适的规范 ）| 30 | 30 |
 | · Design  | · 具体设计| 150 | 200 |
 | · Coding  | · 具体编码| 500 | 900 |
 |· Code Review  |  代码复审| 30 | 60 |
 | · Test| · 测试（自我测试，修改代码，提交修改）| 30 | 300 |
 | Reporting | 报告| 30 | 30 |
 | · Test Repor | · 测试报告| 30 | 30 |
 | · Size Measurement  |  计算工作量| 20| 20 |
 | · Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划| 30 | 50 |
 | |· 合计| 1500 | 2790 |
# 二. 任务要求的实现
## 项目设计与设计栈
主要分成了三个部分，爬虫与数据分析与数据可视化。
为了实现爬虫功能，绕开wjw官网的反爬机制，主要运用了Pyppeteer这个python库。
为了实现数据分析功能，主要运用了xlwings这个python制作Excel表格的库函数，并且用正则表达式中的search功能对爬取下来的文本文档进行分析，提取我们需要的文本。在这个项目中就是全国的新增本土病例和各省的新增本土病例，由于卫健委的每日新增报告从2020年到现在文本不一，并没有一个统一的规范，这也就成了我们处理文本的一大难点。
为了实现数据的可视化功能，主要运用了pandas这个python库，制作横轴为时间，纵轴为全国当日新增本土病例的折线图。同时也尝试了任选某一日的全国各省新增病例地图。这主要运用Timeline,Map这两个python的组件库。
## 爬虫与数据处理
主要运用了pyppeteer这个python的爬虫库绕开反爬机制。
以及用beautifulsoup的爬虫库进行数据处理，解析网页。
主要运用以下几个函数

 fetchUrl 函数：用于发起网络请求，获取网页源码
 getPageUrl 函数：构造每一页的 URL 链接
 getTitleUrl 函数：获取某一页的文章列表中的每一篇文章的标题，链接，和发布日期
 getContent 函数：获取某一篇文章的正文内容
  saveFile 函数：将爬取到的数据保存在本地的 txt 文档里
re.match函数：对文本文档中的数据进行正则处理，找到我们需要的文本内容。
由于爬取的内容较多，有足足43页，故我将range设置为分成三次爬取，以防出现什么故障问题。完成爬虫这一步后，就初步对卫健委官网的数据进行了处理，爬取到本地文本文档之中。需要注意的是，由于卫健委的反爬机制很强，如果用传统的selenium这一库则一般会被识别为爬虫无法进行爬取，所以采用pyppeteer这个库非常重要，我也是在多次失败阅读了大量博客后才发现了这个爬虫神器。

而后我们使用re.match函数和re.search函数对文本文档中的数据进行处理，找到本土病例和各地区病例，排除境外输入病例与疑似病例。在本项目中要分为多次查找，例如“本土病例”“港澳台地区通报确诊病例”就是我们需要去正则查找的字符串。另外由于文本里港澳台地区只有通报的确诊病例之和而没有每日新增病例后续还要进行减法的算法处理，也是非常难处理的一个算法。

正则处理结束后，数据的初步处理就已经完成，之后我们要将处理好的数据写入Excel文件中，采取的办法是导入xlwt和sqlite3这两个python库。
其中xlwt可以进行Excel操作生成表格，sqlite3可以进行数据库操作。
![在这里插入图片描述](https://img-blog.csdnimg.cn/bc183e891fd6416ab335badd71cd6f6d.png#pic_center)
如图就是爬取的excel的情况，设置了地区和日期以及新增确诊人数，主要制成了两个excel一个是全国的日期与当日新增确诊和各地区的当日新增确诊。
不过至此爬虫与数据处理部分就算初步完成。
## 数据统计接口部分的性能改进
数据统计接口上花费的时间较长，爬了2小时多才将疫情以来的数据爬入文本文档中，经过分析getContent 函数和getPageUrl 函数是耗时最久的。查阅资料后，为了进行改进我利用了cpu的多核系统，进行多线程爬虫，提高了数据爬取的速度。时间由2小时多缩短到了1小时左右，有了很大进展。

## 每日热点的实现思路
每日热点实现主要是从数据变化量上出发，在Excel中将全国各地的每日新增病例进行可视化后，与前一日进行对比，如果某省的新增病例增加了100%以上，则认为该地出现了疫情爆发，是当日的热点。
具体算法上主要采用pandas库进行操作，从Excel中将每日的数据与前一日进行对比得出每日热点。算法的优缺点上，优点是直接进行数据分析较为方便，缺点是没有使用更好的函数来提高效率。改进上还是主要放在寻找更好的函数上。另外就是全国疫情的一个每日热点，由于全国疫情人口基数大，所以设定全国新增病例要增加500%以上才认为出现了一个全国疫情的爆发点。
## 数据可视化界面的展示
对数据的可视化，主要还是利用pandas这一python库。同时还有pyecharts这个库来进行折线的绘制，和坐标轴的形成。
首先我们用time函数对时间进行先后排序，将日期排序并制成列表，而后我们用line函数来绘制折线图，配置好x轴与y轴，以及曲线的平滑程度，线条宽度和样式，点的大小，图例名称，坐标轴两边是否留取空白这些函数可选项。这些都可以通过line函数来实现。

line = Line().add_xaxis(
                      xaxis_data = time   # 输入x轴数据
                      )
前期日期爬取部署好后，输入这个代码，即可将日期转化为横轴。
 line.add_yaxis(
             series_name = i,  
             y_axis = alltime_china[i],  # 输入y轴数据 
输入此代码则可将病例转为y轴，我们只需要输入新增本土病例即可，前面爬取的治愈病例等则不需要，后期可视化大屏再看如何处理他们这些数据。

可视化界面主要就由人数构成的y轴，日期构成的x轴，和平滑曲线展示，每个点代表着当日全国的新增病例。

# 三.心得体会
本次作业中，我第一次接触了python这一语言，并去尝试爬虫与数据处理。可以说作业难度对我这样一个第一次接触python与爬虫的小白来说是非常非常非常非常非常大的。所以我主要还是在查阅全网各式各样的博客进行研究并询问大佬同学，自己写出爬虫的初步代码，对python语言里常出现的错误进行规避，模仿博客大佬们的代码写法，对自己代码里的不足进行修改，同时将他们代码研究后改为适用于我这个项目的版本，并对修改后代码里出现的错误进行修改debug。本次作业中在翻阅博客学习别人的思路和代码花费了很多时间，当然自己写的爬虫代码也花去了大量时间，因为第一次接触python这个语言所以代码里的bug非常多，删删改改查查错误原因也花去了大量时间，有时候不知道错误在哪几乎快把百度翻烂了才最终找到错误原因。

三个模块中，学习最深入的就是爬虫部分，我学会了如何构造url链接，分析网页信息的模块，并基于此对网页内容进行爬取，同时还了解了很多虽然没怎么用上但是也是爬虫里很重要的概念，例如构造header头来伪装自己让网站识别不出来是爬虫，以及采取更先进的pyppeter库来绕过反爬机制。在爬虫部分我初步学习后基本有了自己的代码思路，代码的完成主要依靠自己以及询问同学，但是之后的数据可视化处理实在是一脸懵不知从何下手，所以我基本是对各个博客的代码进行学习研究，基本弄明白他们的代码原理后再对代码进行增改删查，适用到自己的代码中，但是适用的时候也有一堆适配错误让我删删改改了半天，不过这样也提升了对代码的一个理解。

要说还有什么别的收获就是锻炼了自己的心态，一个是调试的过程中不断有代码出错，或者是没有下载什么模块，或者是pycharm没有加载到python下载的块，或者是具体语法在改动过程中出现了什么问题，在这一过程中我逐渐认识到了代码修改过程中可能会出现的错误，进一步加强了代码能力。同时也对python的各个库有了基本的了解，对以后尝试这方面的开发有了初步的帮助。另一个就是发现必须静下心来去把别人代码里的原理弄明白了，搞清楚函数和库的原理和用途进行修改的时候才不会出现一堆bug，如果心态特别急躁只是一味的复制粘贴，一堆的错误以及和自己项目里的数据根本不匹配更让人抓狂。所以静下心来去主动学习别人代码的原理是非常重要的，看博客也不是只是看和copy，要去学习原理才能真正用到自己的代码上。欲速则不达，急着copy只会看着一堆bug然后更加无从下手了。

另外最主要的还是推动了自己的主动学习吧，这样一个庞大的项目确实是需要自己不断地去自主学习的，自主学习我想也是程序员一生不可避免的一课，提前接触当然也是有很大帮助的。这样一个项目让我在学校第一次有了走出象牙塔面向社会的感受，还是非常有意义的，我也在此过程中受益匪浅，虽然还有很多需要提升的地方，但是我相信总会做好的。

